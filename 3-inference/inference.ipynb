{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demo using Sagemaker inference in BYOC mode, so first we need package our container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using AWS Deep Learning Container as our base container, you can check the available list in https://aws.amazon.com/cn/releasenotes/available-deep-learning-containers-images/\n",
    "\n",
    "Remember change the base container by the region you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Container build有两种方式（二选一）\n",
    "\n",
    "## 1. 自己构建（在中国区会较慢）\n",
    "\n",
    "## 2. 使用现有的(推荐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only onece to create the repository in ECR\n",
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'spoken-language-identification-sagemaker-inference-container'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "inference_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "print(inference_repository_uri)\n",
    "ecr = '{}.dkr.ecr.{}.{}'.format(account_id, region, uri_suffix)\n",
    "\n",
    "!aws ecr create-repository --repository-name $ecr_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 自己构建（在中国区会较慢）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "# https://aws.amazon.com/cn/releasenotes/available-deep-learning-containers-images/\n",
    "# FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:1.15.2-cpu-py36-ubuntu18.04\n",
    "FROM 727897471807.dkr.ecr.cn-north-1.amazonaws.com.cn/tensorflow-training:1.15.2-cpu-py36-ubuntu18.04\n",
    "\n",
    "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
    "         wget \\\n",
    "         nginx \\\n",
    "         libsm6 \\\n",
    "         libxrender1 \\\n",
    "         libglib2.0-dev \\\n",
    "         libxext6 \\\n",
    "         libsndfile1 \\\n",
    "         libsndfile-dev \\\n",
    "         libgmp-dev \\\n",
    "         libsox-dev \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# RUN mkdir /opt/ml/code\n",
    "WORKDIR /opt/ml/code\n",
    "COPY slr ./\n",
    "\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install -r requirements.txt \n",
    "#-i https://mirrors.163.com/pypi/simple/\n",
    "\n",
    "# Here we get all python packages.\n",
    "# There's substantial overlap between scipy and numpy that we eliminate by\n",
    "# linking them together. Likewise, pip leaves the install caches populated which uses\n",
    "# a significant amount of space. These optimizations save a fair amount of space in the\n",
    "# image, which reduces start up time.\n",
    "\n",
    "RUN pip install flask gevent gunicorn boto3 && \\\n",
    "        rm -rf /root/.cache\n",
    "\n",
    "WORKDIR /opt/\n",
    "RUN wget https://johnvansickle.com/ffmpeg/builds/ffmpeg-git-amd64-static.tar.xz && xz -d ffmpeg-git-amd64-static.tar.xz \\\n",
    "    && tar xvf ffmpeg-git-amd64-static.tar \n",
    "WORKDIR /opt/ffmpeg-git-20200617-amd64-static\n",
    "RUN cp ffmpeg  ffprobe  qt-faststart  /usr/bin/\n",
    "    \n",
    "WORKDIR /opt/\n",
    "RUN wget http://downloads.xiph.org/releases/ogg/libogg-1.3.4.tar.gz \\\n",
    "    && tar -zvxf libogg-1.3.4.tar.gz \n",
    "WORKDIR /opt/libogg-1.3.4 \n",
    "RUN ./configure && make && make install\n",
    "    \n",
    "WORKDIR /opt/\n",
    "RUN wget http://downloads.xiph.org/releases/vorbis/libvorbis-1.3.6.tar.gz \\\n",
    "    && tar -zvxf libvorbis-1.3.6.tar.gz\n",
    "WORKDIR /opt/libvorbis-1.3.6 \n",
    "RUN ./configure && make && make install\n",
    "    \n",
    "WORKDIR /opt/\n",
    "RUN wget https://ftp.osuosl.org/pub/xiph/releases/flac/flac-1.3.3.tar.xz \\\n",
    "    && xz -d flac-1.3.3.tar.xz \\\n",
    "    && tar xvf flac-1.3.3.tar\n",
    "WORKDIR /opt/flac-1.3.3 \n",
    "RUN ./configure && make && make install \\\n",
    "    && ln -s /usr/local/bin/flac /usr/bin/flac\n",
    "    \n",
    "WORKDIR /opt/    \n",
    "RUN wget https://jaist.dl.sourceforge.net/project/sox/sox/14.4.2/sox-14.4.2.tar.gz \\\n",
    "    && tar -zvxf sox-14.4.2.tar.gz\n",
    "WORKDIR /opt/sox-14.4.2 \n",
    "RUN ./configure \\\n",
    "    && make && make install \\\n",
    "    && ln -s /usr/local/bin/sox /usr/bin/sox \\\n",
    "    && ln -s /usr/local/bin/soxi /usr/bin/soxi\n",
    "\n",
    "\n",
    "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n",
    "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n",
    "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n",
    "# PATH so that the train and serve programs are found when the container is invoked.\n",
    "WORKDIR /opt/ml/code\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "ENV PATH=\"/opt/ml/code/:${PATH}\"\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ecr get-login-password --region cn-north-1 | docker login --username AWS --password-stdin 727897471807.dkr.ecr.cn-north-1.amazonaws.com.cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $ecr\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository ./\n",
    "\n",
    "!docker tag {ecr_repository + tag} $inference_repository_uri\n",
    "!docker push $inference_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 使用现有的docker镜像"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将现有镜像下载到本地，并推送到自己的ECR库中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要改下面这行命令\n",
    "!aws ecr get-login-password --region cn-north-1 | docker login --username AWS --password-stdin 346044390830.dkr.ecr.cn-north-1.amazonaws.com.cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exist_image = '346044390830.dkr.ecr.cn-north-1.amazonaws.com.cn/spoken-language-identification-sagemaker-inference-container:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull $exist_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $ecr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $exist_image $inference_repository_uri\n",
    "!docker push $inference_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference（两种方式均可）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意\n",
    "\n",
    "**将如下model_uri改为在training阶段得到的模型在S3中的path，形式为s3://YOUR_BUCKET_NAME/spoken/output/tensorflow-training-x-x-x-x-x-x-x/output/model.tar.gz**， 可以在console找到该训练任务，在该训练任务的描述页面中，找到“S3 模型构件”，复制即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = inference_repository_uri\n",
    "# update model_uri to your model S3 uri\n",
    "model_uri = 'YOUR_MODEL_URI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "推理请求的结构是发送一个json结构体，json结构体里面描述：\n",
    "\n",
    "bucket: 存放待推理音频数据的存储桶\n",
    "\n",
    "audio_uri:待推理音频数据在S3的uri，不含桶名\n",
    "\n",
    "class_count: 语音语言种类，与模型训练时强相关，即模型训练的时候提供了几种语言的种类，这儿就填几，如训练时提供了5种语言，这里就写5；\n",
    "\n",
    "**即发送推理请求前，先将待推理的音频文件上传到S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# bucket为保存待推理的音频文件桶名，audio_uri为该待推理文件在S3中的uri，且不含有桶名，即只有前缀，如一个名为demo1.mp3文件上传到桶名为test的s3存储桶后（且audio目录下），\n",
    "# 则该文件的S3 uri为s3://test/audio/demo1.mp3, 其中test为桶名，audio/demo1.mp3即为下面audio_uri的值；\n",
    "bucket = 'YOUR_BUCKET_SOTRE_AUDIO_TO_INFERENCE'\n",
    "audio_uri = 'xxxxxxxx.mp3'\n",
    "\n",
    "test_data = {\n",
    "    'bucket' : bucket,\n",
    "    'audio_uri' : audio_uri,\n",
    "    'class_count' : '3'\n",
    "}\n",
    "payload = json.dumps(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using sagemaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below could be modified as you want\n",
    "\n",
    "initial_instance_count = 1\n",
    "instance_type = 'ml.m5.large'\n",
    "endpoint_name= 'spl-endpoint-July'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 model\n",
    "\n",
    "from sagemaker.model import Model\n",
    "image = inference_repository_uri\n",
    "tfModel =Model(\n",
    "            model_data=model_uri, \n",
    "            role=role,\n",
    "            image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 endpoint\n",
    "\n",
    "tfModel.deploy(\n",
    "    initial_instance_count=initial_instance_count,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建推理用的 predictor\n",
    "\n",
    "new_predictor = sagemaker.predictor.RealTimePredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    content_type='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理请求代码\n",
    "\n",
    "new_sm_response = new_predictor.predict(payload)\n",
    "\n",
    "print(json.loads(new_sm_response.decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using boto3 SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below could be modified as you want\n",
    "\n",
    "model_name = 'spl-demo'\n",
    "endpoint_config_name='spl-endpoint-config-July'\n",
    "variant_name= 'spl-variant-1-June'\n",
    "initial_instance_count = 1\n",
    "instance_type = 'ml.m5.large'\n",
    "endpoint_name= 'spl-endpoint-July'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# create model object\n",
    "\n",
    "spl_model_demo = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': image,\n",
    "        'Mode': 'SingleModel',\n",
    "        'ModelDataUrl': model_uri,\n",
    "    },\n",
    "    ExecutionRoleArn= role, \n",
    "    EnableNetworkIsolation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create endpoint config\n",
    "\n",
    "spl_endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': variant_name,\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': initial_instance_count,\n",
    "            'InstanceType': instance_type\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create endpoint\n",
    "\n",
    "response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "待上一步创建完成后再进行下面的发送推理请求。上面创建endpoint的时间大概10分钟左右，可以在console查看状态，inservice即可使用了。\n",
    "\n",
    "推理代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "region_name='cn-north-1'\n",
    "profile_name='default'\n",
    "\n",
    "session = boto3.session.Session(region_name=region_name, profile_name=profile_name)\n",
    "client = session.client('sagemaker-runtime')\n",
    "\n",
    "start_time = time.time()\n",
    "spl_response=client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "        Body=payload,\n",
    "        ContentType='application/json')\n",
    "end_time = time.time()\n",
    "\n",
    "print('time cost %s s' %(end_time - start_time))\n",
    "print(json.loads(spl_response['Body'].read().decode()))"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
